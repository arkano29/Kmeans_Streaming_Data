{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"Streaming_KMeans.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"F8ufucarDGCt"},"source":["## Source Code of the paper \"Passive Approach for the K-means Problem on Streaming Data\" ##"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFT4S6JqDGCx"},"source":["EficiencyExperiment = True\n","SurrogateExperiment = True\n","\n","MAIN = \"\" # MAIN DIRECTORY #"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Xe42iYCDGCy"},"source":["### IMPORTS"]},{"cell_type":"code","metadata":{"id":"iFw3-1uXDGCz"},"source":["## Make sure to have all dependencies installed ##\n","\n","#Requirements made with pipreqs or pipreqsnb\n","import os\n","import math\n","import scipy\n","import time\n","import pickle\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Patch\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import MiniBatchKMeans\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import MDS\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9vnDP5JSDGC0"},"source":["## Here the directories for the results are generated, some folders are made on the directory were the folder Notebooks is ##\n","\n","def parent(path):\n","    return os.path.split(path)[0]\n","def sign(x):\n","    if x:\n","        return abs(x)/x\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","path = os.getcwd()\n","os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\"))) #Here results of the main experiments are stored\n","os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Surrogate\"))) #Here results of the surrogate experiment are stored"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7TkJTnhKDGC2"},"source":["### Define error function, distance and label assignment"]},{"cell_type":"code","metadata":{"id":"3c1ay4WlDGC3"},"source":["## Some important functions are defined for experiments ##\n","\n","def compute_labs(x,centers): #Given a set of points and a set of centroids returns a list of labels\n","    labs = np.zeros(shape=(x.shape[0]))\n","    for i in range(x.shape[0]):\n","        labs[i]=np.argmin(np.linalg.norm((x[i,:]-centers),axis=1))\n","    return (labs)\n","\n","def distance(x,centers,compute_labs = False): #Computes a list of the distances from each point to each nearest centroid, if requested also returns a list of labels\n","    dis = np.zeros(shape=(x.shape[0]))\n","    if compute_labs:\n","        labs = np.zeros(shape=(x.shape[0]))\n","        for i in range(x.shape[0]):\n","            aux = np.linalg.norm((x[i,:]-centers),axis=1)\n","            dis[i] = aux.min()\n","            labs[i] = np.argmin(aux)\n","        return (dis,labs)\n","    else:\n","        for i in range(x.shape[0]):\n","            dis[i] = np.linalg.norm((x[i,:]-centers),axis=1).min()\n","        return (dis)\n","\n","def distance_labs(x,centers,labs): #This functions does the same but the labels are allready given, which speeds up computations\n","    dis = np.zeros(shape=(x.shape[0]))\n","    for i in range(x.shape[0]):\n","            dis[i] = np.linalg.norm((x[i,:]-centers[labs[i],:]))\n","    return (dis)\n","\n","def inertia(x,centers,weight = np.array([None]),labs = np.array([None])): #Returns the K-means error(weighted if weight is given) for a dataset and centroids\n","    if any(weight==None):\n","        weight=np.ones(shape=(x.shape[0]))\n","    if any(labs!=None):\n","        error = np.sum(weight*(distance_labs(x,centers,labs))**2)\n","    else:\n","        error = np.sum(weight*(distance(x,centers))**2)\n","    return (error/np.sum(weight))\n","\n","def print_and_log(log_file,text): #If wanted to log the experiment steps, this function prints a text to the log file and to the screen\n","    print(text,file=log_file)\n","    os.system(\"echo \"+str(text))\n","    \n","def max_order(a): #Helps debugging, returns the maximum order of magnitude(in base 10) of the values in an array\n","    return round(np.log(a.max()+1.0e-10)/np.log(10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FpaTVwjDGC4"},"source":["### Read datasets"]},{"cell_type":"code","metadata":{"id":"EQI9NoNmDGC5"},"source":["## Here an object is defined in order to read datasets ##\n","\n","class Dataset:\n","    \n","    \"Data reader\"\n","    #File are expected to be in .csv format, a single file can be given or a folder containing many datasets(with the same number of columns). Variables should be numerical, so in order to remove \n","    #class variables or any non interesting variables insert a list of indexes corresponding to the columns to be removed. If the data was already stored as a Python object, it can be stated with the last input variable\n","    \n","    def __init__(self,file = None,folder = None,class_index = [],header = \"infer\",is_python_object=False): \n","        \n","        if is_python_object:\n","            with open(file, 'rb') as input:\n","                self.data= pickle.load(input)\n","        else:\n","            if file:\n","                df = pd.read_csv(file,header = header) #Reads .csv file using panda's dataframe\n","                #df = df.fillna(axis = 1,method = \"bfill\") #If there are missing values uncomment this line, and use the desired filling method\n","                self.classes = df[df.columns[class_index]] #Creates the list of columns to be removed\n","                df.drop(labels = df.columns[class_index],axis = 1,inplace = True) #Remove the undesired columns\n","                self.names = list(df.columns) \n","                self.data = df.to_numpy()                \n","                del df #Delete dataframe\n","                \n","            #In this case the same procedure is conducted, but a list of .csv files are concatenated instead\n","            elif folder:\n","                df = pd.DataFrame()\n","                for f in os.listdir(folder):\n","                    if \".csv\" in f:\n","                        df = df.append(pd.read_csv(os.path.join(folder,f),header = header))\n","                #df = df.fillna(axis = 1,method = \"bfill\")\n","                self.classes = df[df.columns[class_index]]\n","                df.drop(labels = df.columns[class_index],axis = 1,inplace = True)\n","                self.names = list(df.columns)\n","                self.data = df.to_numpy()\n","                del df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlCZUYgrDGC6"},"source":["### Create Concept Drift"]},{"cell_type":"code","metadata":{"id":"jOxHwZQ9DGC6"},"source":["## Here the function that generated controlled (1+epsilon)-drifts is given ##\n","\n","#The number of clusters must be given in order to compute a set of centroids. The number of concepts that must be generated can be given, if the e_list contains more than one value then the concepts are generated according \n","#to the list,but if e_list has a single value then n_concepts are generated with the epsilon value given on the list. The file/folder of the original dataset is given, and where the output streaming data is stored must be given.\n","#The tolerance of the heuristic procedure is set default to 0.05, and the class_index must be given as well as when a dataset was read.\n","\n","def Generate_concept(K = None, n_concepts = None, e_list= [None],file_in = None, file_out = None, tol = 0.05,class_index = []):\n","    \n","    if len(e_list)==1:\n","        aux = e_list\n","        e_list = [aux[0] for _ in range(n_concepts)]\n","        del aux\n","    else:\n","        n_concepts = len(e_list) #If e_list has more than one value then n_concept is the size of the list\n","\n","    #The dataset is read\n","    if os.path.isfile(file_in):\n","        data = Dataset(file = file_in,class_index = class_index).data\n","    else:\n","        data = Dataset(folder = file_in,class_index = class_index).data\n","    \n","    \n","    data = data[np.random.shuffle(np.arange(data.shape[0])),:][0] #Shuffle the data\n","    if data.shape[0]>100000: #For massive data, select only 100.000 data points maximum\n","        data = data[0:100000,:]\n","    \n","    init_tol = tol #The tolerance will increase as more iterations are run, in order to generate data in reasonable time\n","    cd_size = data.shape[0] #The concept drifts will be generated with the whole original dataset, then the number of points on each concept is the number of points from the original dataset\n","\n","    i = 0\n","    current_data = data[0:cd_size,:]\n","    out_data = current_data\n","    \n","    skm = KMeans(n_clusters = K,n_init = 1,init = \"k-means++\",max_iter = 100) #Define the K-means problem\n","    skm.fit(current_data) #Compute the centroids C_{1}\n","    \n","    for e in e_list:\n","        i+=1\n","        \n","        current_error = skm.inertia_/current_data.shape[0] #Compute E(X_{i-1},C_{i-1})\n","        current_centers = skm.cluster_centers_ #C_{i-1}\n","        labels = skm.labels_\n","        \n","        valid = False\n","        \n","        maximum_random = 50 #Maximum of random directions considered\n","        maximum_mag = 20 #Maximum number of magnitudes computed(alpha^*_i)\n","        j = 1\n","        tol = init_tol #Set tolerance to the initial value\n","        \n","        while j<maximum_random and not valid:\n","            \n","            random = np.random.random(size=(K,current_data.shape[1])) #Set a random direction for each cluster\n","            rand = np.divide(random,np.linalg.norm(random,axis = 1).reshape(random.shape[0],1)) #Normalize directions\n","            alpha = math.sqrt(e*current_error/(K*cd_size)) #Set initial value for alpha\n","            j+=1\n","            k = 0\n","            while k<maximum_mag and not valid:\n","                \n","                k+=1\n","                random = alpha*rand #Scale random directions\n","                traslated_data = current_data.copy()\n","                \n","                for lab in range(K):\n","                    traslated_data[np.where(labels==lab),:] = traslated_data[np.where(labels==lab),:] + random[lab,:] #Traslate each cluster\n","                \n","                new_error = inertia(traslated_data,current_centers) #Compute the new error\n","                print(\"Error ratio= %s\"%(new_error/current_error)) #For log\n","                per = ((new_error/current_error-1)-e)/e #Compute ratio with respect to the desired error\n","                print(per*100)\n","                \n","                if (abs(per)>1 and k>2): #If the error differs more than a 100% with respect to the desired one, compute other random directions\n","                    print(\"Change direction\")\n","                    break\n","                if abs(per)<tol: #If the difference is below the threshold, then it is a valid traslation and the loop is ended\n","                    valid = True                    \n","                else:\n","                    alpha-=(new_error/((1+e)*current_error)-1)*math.sqrt(e*current_error/(K*cd_size)) #If the difference is not below the threshold, update alpha \n","            tol+=0.01\n","        if valid:\n","            print(\"Success\")\n","            skm = KMeans(n_clusters = K,init = skm.cluster_centers_,max_iter = 100)\n","            skm.fit(traslated_data) #Compute new centroids C_{i}\n","            out_data = np.concatenate((out_data,traslated_data)) #Concatenate the generated data\n","            current_data = traslated_data.copy()\n","        else:\n","            print(\"Not succeded\")\n","        print(e)\n","    \n","    with open(file_out, 'wb') as output:\n","        pickle.dump((cd_size,out_data), output, pickle.HIGHEST_PROTOCOL) #Save the generated Streaming data as a python object\n","    \n","    #Remove data \n","    del data \n","    del current_data\n","    del traslated_data\n","    del out_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVl6Q7LdDGC7"},"source":["## Here some dictionaries and lists are used to determine how to generate the streaming data for each dataset ##\n","\n","#A dict containing the class_indexes for each dataset\n","index_dict = {\"Urban\":[],\"Google\":[0],\"Postures\":[0,1],\"SUSY\":[0],\"Gas\":[0],\"Pulsar\":[8],\"MiceProtein\":[0,-4,-3,-2,-1],\"Frogs\":[0,-4,-3,-2,-1],\"Epilepsia\":[0,-1],\"Gesture\":[-2,-1]} \n","#A list of used number of clusters to generate different streaming data\n","k_list = [5,10,25,50]\n","#A dictionary with the empirical optimal values of K for each dataset\n","k_dict = {\"SUSY\":2,\"Pulsar\":2,\"Epilepsia\":5,\"Urban\":400,\"Frogs\":60, \"Google\": 10,\"MiceProtein\":10,\"Gesture\":10,\"Gas\":5}\n","#This boolean determines if the number of clusters is used from the list or using the dictionary\n","k_from_list = True\n","#The names of the dataset used to generate different streamming data\n","names = [\"Urban\",\"SUSY\",\"Pulsar\",\"Epilepsia\",\"Google\",\"Frogs\",\"Gesture\",\"Gas\"]#\"MiceProtein\"\n","#Dictionary containing the input file/folder of each dataset\n","direc_dict = {\"Urban\":\"urbanGB.txt\",\"Google\":\"google_review_ratings.csv\",\"Postures\":\"Postures.csv\",\"Frogs\":\"Frogs_MFCCs.csv\",\"SUSY\":\"SUSY.csv\",\"Gas\":\"gas-sensor-array-temperature-modulation\",\"Gesture\":\"gesture_phase_dataset\",\"Pulsar\":\"HTRU_2.csv\",\"MiceProtein\":\"MiceProtein.csv\",\"Epilepsia\":\"Epilepsia.csv\"}\n","#List of epsilon values used, for this experiment n concepts are generated with the same epsilon value, hence each value defined in this list determines different experiments\n","e_list = [0.5,1,2]\n","#Dictionary determining the string representation of the epsilon values used on the name when saving the output\n","e_dict = {0.5:\"05\",1:\"1\",2:\"2\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9DOu5RzZDGC8"},"source":["generate = False #Whether to generate or not\n","\n","if generate:\n","    for k in k_list:\n","        for e in e_list:\n","            for name in names:\n","                if not k_from_list:\n","                    k = k_dict[name]\n","                Generate_concept(K = k, n_concepts = 9, e_list= [e],file_in = os.path.join(parent(path),\"Datasets\",direc_dict[name]), \n","                                 file_out = os.path.join(parent(path),\"Python Objects\",name+e_dict[e]+\"k=\"+str(k)), tol = 0.05,class_index = index_dict[name])\n","                print(\"Done with %s for K=%s and e=%s\\n\"%(name,k,e))\n","\n","#Thse lines below define isolated generations\n","#e_list = [0.5]#[0.1,0.2,0.5,1,2,10]\n","#Generate_concept(k = 13, n_concepts = 9, e_list= e_list,file_in = os.path.join(MAIN,\"Datasets/gas-sensor-array-temperature-modulation\"), file_out = os.path.join(MAIN,\"Python Objects/SimulatedData05\"), tol = 0.1)\n","#Generate_concept(k = 1, n_concepts = 1, e_list= e_list,file_in = os.path.join(MAIN,\"Datasets/gas-sensor-array-temperature-modulation\"), file_out = os.path.join(MAIN,\"Python Objects/SimulatedData1\"), tol = 0.06)\n","#Generate_concept(k = 13, n_concepts = 9, e_list= e_list,file_in = os.path.join(MAIN,\"Datasets/SUSY.csv\"), file_out = os.path.join(MAIN,\"Python Objects/AritzCDSUSY\"), tol = 0.05)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EgrM0hVKDGC8"},"source":["### Stream generator"]},{"cell_type":"code","metadata":{"id":"zO-2PWqDDGC8"},"source":["## This Pyhton object generates an iterator that returns batches of data with controlled concept drift ##\n","\n","class StreamCD:\n","\n","    \"\"\"Iterator that returns batches of data\"\"\"\n","    \n","    #Input file with the streaming data(in our case a python object), determine the batch size and the desired period of each concept drift. \n","    #First batches file contains the original dataset and is used to burn out the stored batches before making measurements, tmax determines \n","    #how many batches untill the burn out. Class index is used again for the original dataset.\n","    \n","    def __init__(self,file = \"\",batch_size = None,cd_period = None,first_batches=\"\",tmax = 0,class_index = []):\n","        \n","        with open(os.path.join(file), 'rb') as input:\n","            \n","            (concept_size,data)=pickle.load(input) #Load streaming data with the concept drift size(original data size)\n","        \n","        if first_batches: #If stated, read first batches\n","            if os.path.isfile(first_batches):\n","                self.initial_data = Dataset(file = first_batches,class_index = class_index).data\n","            else:\n","                self.initial_data = Dataset(folder = first_batches,class_index = class_index).data\n","                \n","        self.tmax = tmax\n","        self.cd_size = concept_size\n","        self.data = data\n","        self.batch_size = batch_size\n","        self.max = data.shape[0]\n","        self.num = 0\n","        self.concept = 0\n","        self.cd_period = cd_period\n","        self.i = 0\n","        self.initial = True\n","        self.swap = True\n","        \n","        if self.cd_period and not self.batch_size: #If the period is stated but not the batch size then compute it such that the maximum number of data is extracted\n","            self.batch_size = self.cd_size//self.cd_period\n","            \n","        if self.batch_size*self.cd_period>self.cd_size: #The data is extracted in order without replacement, so we can not extract more data than we have\n","            print(\"Imposible\")\n","            raise StopIteration\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        \n","        cd = False\n","        if self.i>self.tmax and self.swap: #If tmax batches have already passed, swap to the streaming data with concept drifts\n","                self.initial = False\n","                self.num = 0\n","                self.i = 0\n","                self.swap = False\n","                \n","        if self.num+self.batch_size>self.initial_data.shape[0]: #If more batches are needed from the original data until the burn out, then restart from the beginning\n","            self.num = 0\n","                \n","        if self.initial: #Return batches from the original dataset\n","            batch = self.initial_data[self.num:self.num+self.batch_size,:]\n","            self.num+=self.batch_size\n","            self.i+=1\n","        \n","        if not self.initial: \n","            \n","            if self.cd_period:#If cd_period is stated, then return batches with concept drift each period\n","                if self.i == self.cd_period:\n","                    self.concept+=1\n","                    self.num = self.concept*self.cd_size\n","                    self.i = 0\n","                    cd = True\n","\n","                if self.num>=self.data.shape[0]: #If all batches have been returned then stop iteration\n","                    raise StopIteration\n","\n","                self.i+=1\n","                aux = self.data[self.concept*self.cd_size:(self.concept+1)*self.cd_size,:]\n","                np.random.seed(int(10000*time.time())%(2**31))\n","                batch = aux[np.random.choice([i for i in range(aux.shape[0])],self.batch_size,False),:] #Choose a batch randomly\n","                self.num+=self.batch_size\n","\n","            else:#If cd_period is not stated, then return as many batches as possible from each concept \n","\n","                if self.num//self.cd_size != self.concept:\n","                    self.concept = self.num//self.cd_size\n","                    cd = True\n","                last_index = (self.concept+1)*self.cd_size\n","\n","                if self.num>=self.data.shape[0]:\n","                    raise StopIteration\n","\n","                if self.num+2*self.batch_size>last_index-1:\n","                    batch = self.data[self.num:self.num+self.batch_size,:]\n","                    self.num+=last_index\n","\n","                else:\n","                    batch = self.data[self.num:self.num+self.batch_size,:]\n","                    self.num+=self.batch_size\n","        \n","        return batch,cd #Return a batch and whether a concept drift has occurred or not\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7pZkRlKDGC9"},"source":["### SKMeans"]},{"cell_type":"code","metadata":{"id":"HKRCspygDGC9"},"source":["## Define some useful functions for the experiments ##\n","\n","def dis_init_kmplus(n,k): #Returns the number of computed distances when initializing with KM++\n","    return n*k*(k-1)/2-((k-1)**3)/3-((k-1)**2)/2-(k-1)/6\n","\n","def Mean(x,ax): #Computes the mean value through an axis(ax)\n","    x = x\n","    if x.shape[ax]:\n","        return np.sum(x,axis=ax)/x.shape[ax]\n","    else:\n","        return np.zeros(x.shape[ax-1])\n","    \n","def kplus(X, K,w = [False]): #Computes the KM++ initialization, returns a set of initial Centroids\n","    np.random.seed(2020)            #Fixing seeds, so that every initialization(UPC,ICB,HI and WKI) computes the same C^0\n","    C = [X[np.random.choice(range(X.shape[0])),:]]\n","    for k in range(1, K):\n","        if np.array(w).any():\n","            D2 = w*np.array([min([np.inner(c-x,c-x) for c in C]) for x in X])\n","        else:\n","            D2 = np.array([min([np.inner(c-x,c-x) for c in C]) for x in X])\n","        probs = D2/D2.sum()\n","        cumprobs = probs.cumsum()\n","        r = np.random.random()\n","        for j,p in enumerate(cumprobs):\n","            if r < p:\n","                i = j\n","                break\n","        C.append(X[i,:])\n","    return np.array(C)\n","\n","def count(labels,k): #Returns two lists, the first one with all the labels ordered, and the second one with how many of each label is in the input list labels(ordered)\n","    \n","    aux_u,aux_count = np.unique(labels,return_counts=True)\n","    if len(aux_u) == k:\n","        return aux_u,aux_count\n","    else: #If not every label is present, then 0s need to be added to the count list\n","        u = list(range(k))\n","        count = list(range(k))\n","        for lab in u:\n","            if lab in aux_u:\n","                count[lab] = aux_count[np.where(aux_u == lab)][0]\n","            else:\n","                count[lab] = 0\n","        return u,count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEjOtUE-DGC9"},"source":["## Object where the Streaming K-means is carried out, with different methods FSKM(with different initializations) and PSKM ##\n","\n","class SkMeans:\n","    \n","    #Takes an initial batch a executes KM over it, with k clusters. Afterwards FSKM with forget parameter rho or PSKM is executed. With a threshold given, maximum number\n","    #of stored batches is determined with rho, or viceversa. The method determines which initialization technique to use when FSKM is used, or whether to use PSKM. The first \n","    #initialization method is KM++ by default. If wanted to run in multiple threads the number of \n","    #used cores is determined by jobs(default 1). n_init determines if the initialization shall be repeated. mini_batch initialization ban also be used. The variable plot accepts \n","    #a list containing \"pca\" or \"mds\", which will plot a PCA or MDS 2D projection of the datapoints, cmap determines the colormap used for this plot.\n","    \n","    def __init__(self, initial_batch = None, k = 2, rho = None, thresh = None,maxBatch = None,method = None, \n","                 first_init = \"k-means++\",chain_length = 200,afkmc2 = False,jobs = None,n_init = 1,mini_batch_size = None, plot = [\"\"], cmap = \"hsv\"):\n","        \n","        ### Save important parameters\n","        self.k = k\n","        self.first_init = first_init\n","        self.jobs = jobs\n","        self.n_init = n_init\n","        self.mini_batch_size = mini_batch_size\n","        self.chain_length = chain_length\n","        self.afkmc2 = afkmc2\n","        self.method = method\n","        self.dis_init = [0]\n","        \n","        if maxBatch == None:\n","            self.maxBatch = math.ceil(math.log(thresh)/math.log(rho)) #If maxBatch was not given then calculate it\n","        elif rho == None:\n","            rho = thresh**(1./maxBatch) #If rho was not given then calculate it\n","            self.maxBatch = maxBatch\n","        self.rho = np.array([rho**t for t in range(self.maxBatch,-1,-1)])\n","        self.thresh = thresh\n","        self.batches = [initial_batch] #Initiate array of batches\n","        \n","        ### Choose the kmeans method\n","        if self.mini_batch_size:\n","            self.kmeans = MiniBatchKMeans(n_clusters = self.k,n_init = self.n_init,init = self.first_init,batch_size = self.mini_batch_size)\n","        elif self.first_init == \"kmc2\":\n","            self.kmeans = KMeans(n_clusters = self.k,n_init = self.n_init,init = kmc2(np.vstack(self.batches),\n","                            k = self.k, chain_length=self.chain_length, afkmc2=self.afkmc2), n_jobs = self.jobs)\n","            self.dis_init[-1]+= dis_init_kmc2(m = self.chain_length,k = self.k)\n","        else:\n","            init_centers = kplus(self.batches[-1],self.k)\n","            self.error_real_init = [inertia(x = self.batches[-1],centers = init_centers)]\n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","            self.dis_init[-1]+= dis_init_kmplus(n = self.batches[0].shape[0],k = self.k)\n","        \n","        ### Fit to first batch\n","        start = time.time()\n","        self.kmeans.fit(self.batches[0],\n","                        sample_weight = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])]))\n","        self.time = [time.time()-start]\n","        \n","        ### Save results\n","        self.centers = [self.kmeans.cluster_centers_]\n","        self.errors = [self.kmeans.inertia_]\n","        self.error_ratio = [0]\n","        self.t = [0]\n","        self.lloyd_iter = [self.kmeans.n_iter_]\n","        self.dis_lloyd = [self.kmeans.n_iter_*self.k*sum([batch.shape[0] for batch in self.batches])]\n","        self.dis = [self.dis_lloyd[0]+self.dis_init[0]]\n","        self.drift = [False]\n","        self.cardinalities = np.zeros(shape = (self.k))\n","        self.batch_card = np.zeros(shape = (self.k))\n","        self.mean = np.zeros(shape = (self.k,initial_batch.shape[1]))\n","        self.labs = [self.kmeans.labels_]\n","        self.n_instances = [np.vstack(self.batches).shape[0]]\n","        self.error_conver = [self.errors[-1]/self.n_instances[-1]]\n","        self.error_init = []\n","        self.error_real_conver = [self.kmeans.inertia_]\n","        self.all_batches = [initial_batch]\n","        \n","        #If requested plot PCA or MDS projections of the first batch\n","        if \"pca\" in plot:\n","            labels = self.labs[-1]\n","            labels+= 1\n","            labels = labels/np.max(labels)\n","            cm = matplotlib.cm.get_cmap(cmap)\n","            self.pca = PCA(n_components = 2).fit(initial_batch)\n","            projection = self.pca.fit_transform(initial_batch)\n","            plt.scatter(x=projection[:,0],y=projection[:,1],c = cm(labels))\n","            plt.ylabel(\"PC2\",size=20)\n","            plt.xlabel(\"PC1\",size=20)\n","            plt.title(\"PCA projection: t = \"+str(self.t[-1]+1),size=20)\n","            plt.savefig(os.path.join(os.getcwd(),\"Projections\",\"PCA_\"+str(self.t[-1]+1)+\".pdf\"),format=\"pdf\")\n","        \n","        if \"mds\" in plot:\n","            labels = self.labs[-1]\n","            labels+= 1\n","            labels = labels/np.max(labels)\n","            cm = matplotlib.cm.get_cmap(cmap)\n","            self.mds = MDS(n_components = 2).fit(initial_batch)\n","            projection = self.mds.fit_transform(initial_batch)\n","            plt.scatter(x=projection[:,0],y=projection[:,1],c = cm(labels))\n","            plt.ylabel(\"X2\",size=20)\n","            plt.xlabel(\"X1\",size=20)\n","            plt.title(\"MDS projection: t = \"+str(self.t[-1]+1),size=20)\n","            plt.savefig(os.path.join(os.getcwd(),\"Projections\",\"MDS_\"+str(self.t[-1]+1)+\".pdf\"),format=\"pdf\")\n","            \n","    #This inner function is used each time a new batch arrives, and clusters the new batch and previous ones with different methods\n","        \n","    def Kmeans(self,batch,concept_drift = False,forget_centers = False,p=None,cd_index = 1,do_plot = False, plot = [\"\"], cmap = \"hsv\", reset_projection = False,drift = False, hungaro = False,prev_centers = np.array([False])):\n","        \n","        \n","        ### Procedure removing or not previous information, prev_centers is given so that C^* is the same for every FSKM initialization method\n","        \n","        if prev_centers.any() and self.method != \"minimize_real\":\n","            self.centers[-1] = prev_centers\n","            self.labs[-1] = compute_labs(np.vstack(self.batches),self.centers[-1]) #Recompute labels for future calculations\n","        self.all_batches.append(batch)\n","        self.t.append(self.t[-1]+1)\n","        self.drift.append(concept_drift)\n","        self.dis_lloyd.append(0)\n","        self.dis_init.append(0)\n","        self.lloyd_iter.append(0)\n","        \n","        start = time.time() #t_0 to measure elapsed time\n","                \n","        ########### INITIALIZATION ################### \n","        \n","        ## HI initialization ##\n","        \n","        if self.method == \"weights\": \n","            \n","            w_prev = np.zeros((self.k))\n","            i = self.labs[-1].shape[0]\n","            t = 0\n","\n","            while t<len(self.batches):\n","                w = self.rho[-2]**(t+1)\n","                batch_size = self.batches[len(self.batches)-t-1].shape[0]\n","                labs = self.labs[-1][i-batch_size:i]\n","                i-=batch_size\n","                u,c = count(labs,self.k)\n","                for lab in range(self.k):\n","                    w_prev[lab]+=w*c[lab]                    \n","                t+=1\n","\n","            if hungaro:\n","                p = np.zeros((self.k,self.k))\n","                f = np.zeros((self.k,self.k))\n","                previous_cluster_p = w_prev\n","\n","                km = KMeans(n_clusters = self.k,init = kplus(batch,self.k),n_jobs = self.jobs)\n","                km.fit(batch)\n","                self.dis_init[-1]+=dis_init_kmplus(n = batch.shape[0],k = self.k)\n","                self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","                #self.dis_init[-1]+=(len(self.batches)+1)*self.k**2\n","                u,new_cluster_p = count(km.labels_,k = self.k)\n","                \n","                for k in range(self.k):\n","                    for kp in range(self.k):\n","                        p[k,kp]=1/(1+new_cluster_p[kp]/(previous_cluster_p[k]+1.0e-10))\n","                        f[k,kp]+=(new_cluster_p[kp]*p[k,kp]*np.linalg.norm(km.cluster_centers_[kp]-self.centers[-1][k])**2)\n","\n","                a = scipy.optimize.linear_sum_assignment(f)\n","                assignment = a[1]\n","\n","                init_centers = np.zeros((self.k,batch.shape[1]))\n","                for k in range(self.k):\n","                    kp = assignment[k]\n","                    init_centers[k,:]=p[k,kp]*self.centers[-1][k]+(1-p[k,kp])*km.cluster_centers_[kp]\n","\n","            else:\n","                p = np.zeros((self.k,self.k))\n","                previous_cluster_p = np.sum(temp_cluster_sizes,axis = 0)\n","\n","                km = KMeans(n_clusters = self.k,init = self.centers[-1],n_jobs = self.jobs,max_iter = 1)\n","                km.fit(batch)\n","                self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","                u,new_cluster_p = count(km.labels_,k = self.k)\n","                for k in range(self.k):\n","                    kp = k\n","                    p[k,kp]=1/(1+new_cluster_p[kp]/(previous_cluster_p[k]+1.0e-10))\n","\n","                init_centers = np.zeros((self.k,batch.shape[1]))\n","                for k in range(self.k):\n","                    kp = k\n","                    init_centers[k,:]=p[k,kp]*self.centers[-1][k]+(1-p[k,kp])*km.cluster_centers_[kp]            \n","            \n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","        \n","        ## WKI initialization ##\n","        \n","        if self.method == \"centroid_kmeans\":\n"," \n","            w_prev = np.zeros((self.k))\n","            i = self.labs[-1].shape[0]\n","            t = 0\n","            while t<len(self.batches):\n","                w = self.rho[-2]**(t+1)\n","                batch_size = self.batches[len(self.batches)-t-1].shape[0]\n","                labs = self.labs[-1][i-batch_size:i]\n","                i-=batch_size\n","                u,c = count(labs,self.k)\n","                for lab in range(self.k):\n","                    w_prev[lab]+=w*c[lab]                    \n","                t+=1\n","            km = KMeans(n_clusters = self.k,init = kplus(batch,self.k),n_jobs = self.jobs)\n","            km.fit(batch)\n","            self.dis_init[-1]+=dis_init_kmplus(n = batch.shape[0],k = self.k)\n","            self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","            u,w_0 = count(km.labels_,k = self.k)\n","            center_km = KMeans(n_clusters = self.k,init = kplus(np.concatenate((self.centers[-1],km.cluster_centers_),axis = 0),self.k,w=np.concatenate((w_prev,w_0),axis = 0)),n_jobs = self.jobs)\n","            self.dis_init[-1]+=dis_init_kmplus(n = 2*self.k,k = self.k)\n","            center_km.fit(np.concatenate((self.centers[-1],km.cluster_centers_),axis = 0),sample_weight = np.concatenate((w_prev,w_0),axis = 0))\n","            init_centers = center_km.cluster_centers_\n","            \n","            self.dis_init[-1]+= (center_km.n_iter_*2*self.k**2)\n","            \n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","            \n","        ## Update stored batches, and compute indexes of the recent concept ##\n","        if len(self.batches) == self.maxBatch:\n","            del self.batches[0]\n","        self.batches.append(batch)\n","        self.labs[-1] = compute_labs(np.vstack(self.batches),self.centers[-1])\n","        real_index = [x for x in range(-cd_index,0,1)]\n","        \n","        ## PSKM algorithm ##\n","        \n","        if self.method == \"minimize_real\": \n","            if cd_index != 1:\n","                init_centers = self.centers[-1] #Minimizes real SKM error but initializes with previous centroids if a CD did not happen\n","                \n","            else:\n","                km = KMeans(n_clusters = self.k,init = kplus(batch,self.k),n_jobs = self.jobs)\n","                km.fit(batch)\n","                self.dis_init[-1]+=dis_init_kmplus(n = batch.shape[0],k = self.k)\n","                self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","                init_centers = km.cluster_centers_\n","                self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","                self.dis_init[-1]+= dis_init_kmplus(n = batch.shape[0],k = self.k)\n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","            \n","        ## UPC initialization ##\n","            \n","        if self.method == \"prev_centers\": \n","            init_centers = self.centers[-1]\n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)   \n","            \n","        ## ICB initialization ##\n","        \n","        if forget_centers: #If chosen to forget, then for each new batch novel initial centers are computed\n","            if self.mini_batch_size:\n","                self.kmeans = MiniBatchKMeans(n_clusters = self.k,n_init = self.n_init,init = self.first_init,batch_size = self.mini_batch_size)\n","                \n","            elif self.first_init == \"kmc2\":\n","                self.kmeans = KMeans(n_clusters = self.k,n_init = self.n_init,init = kmc2(np.vstack(self.batches),\n","                            k = self.k, chain_length=self.chain_length, afkmc2=self.afkmc2,\n","                            weights = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])])),\n","                            n_jobs = self.jobs)\n","                self.dis_init[-1]+= dis_init_kmc2(m = self.chain_length,k = self.k)\n","                \n","            else:\n","                km = KMeans(n_clusters = self.k,init = kplus(batch,self.k),n_jobs = self.jobs)\n","                km.fit(batch)\n","                self.dis_init[-1]+=dis_init_kmplus(n = batch.shape[0],k = self.k)\n","                self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","                init_centers = km.cluster_centers_\n","                self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","                self.dis_init[-1]+= dis_init_kmplus(n = batch.shape[0],k = self.k)\n","        \n","        \n","        ### LLOYD\n","        ## Fit to the new data ##        \n","        \n","        if self.method == \"minimize_real\": #Lloyd over SKM error\n","            self.kmeans.fit(np.vstack(np.array(self.all_batches)[real_index,:]))\n","        \n","        else: #Lloyd over surrogate error\n","            self.kmeans.fit(np.vstack(self.batches),\n","                            sample_weight = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])]))\n","        \n","        self.time.append(time.time()-start) #Compute elapsed time\n","        \n","        self.error_real_init.append(inertia(x = np.vstack(np.array(self.all_batches)[real_index,:]), centers = init_centers)) #Initial SKM error\n","        self.error_init.append(inertia(x = np.vstack(self.batches),centers = init_centers,weight = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])]))) #Initial surrogate error\n","        \n","        ## Save results ##\n","        \n","        self.labs.append(self.kmeans.labels_)\n","        self.centers.append(self.kmeans.cluster_centers_)\n","        self.errors.append(self.kmeans.inertia_)\n","        self.error_ratio.append(self.errors[-1]/self.errors[-2])\n","        self.dis_lloyd[-1]+=(self.kmeans.n_iter_*self.k*np.vstack(self.batches).shape[0])\n","        self.dis.append(self.dis_lloyd[-1]+self.dis_init[-1])\n","        self.lloyd_iter[-1]+=(self.kmeans.n_iter_)\n","        self.n_instances.append(np.vstack(self.batches).shape[0])\n","        self.error_conver.append(inertia(x = np.vstack(self.batches),centers = self.centers[-1],weight = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])])))\n","        self.error_real_conver.append(inertia(x = np.vstack(np.array(self.all_batches)[real_index,:]), centers = self.centers[-1]))\n","        \n","        if \"pca\" in plot and do_plot: #If stated plot projection\n","            labels = self.labs[-1]\n","            labels+= 1\n","            labels = labels/np.max(labels)\n","            cm = matplotlib.cm.get_cmap(cmap)\n","            if reset_projection:\n","                self.pca = PCA(n_components = 2).fit(np.vstack(self.batches))\n","            projection = self.pca.fit_transform(np.vstack(self.batches))\n","            plt.scatter(x=projection[:,0],y=projection[:,1],c = cm(labels))\n","            plt.ylabel(\"PC2\",size=20)\n","            plt.xlabel(\"PC1\",size=20)\n","            if drift:\n","                plt.title(\"PCA projection(Drift occurred): t = \"+str(self.t[-1]+1),size=20)\n","            else:\n","                plt.title(\"PCA projection: t = \"+str(self.t[-1]+1),size=20)\n","            plt.savefig(os.path.join(os.getcwd(),\"Projections\",\"PCA_\"+str(self.t[-1]+1)+\".pdf\"),format=\"pdf\")\n","        \n","        if \"mds\" in plot and do_plot:\n","            labels = self.labs[-1]\n","            labels+= 1\n","            labels = labels/np.max(labels)\n","            cm = matplotlib.cm.get_cmap(cmap)\n","            if reset_projection:\n","                self.mds = MDS(n_components = 2).fit(np.vstack(self.batches))\n","            projection = self.mds.fit_transform(np.vstack(self.batches))\n","            plt.scatter(x=projection[:,0],y=projection[:,1],c = cm(labels))\n","            plt.ylabel(\"X2\",size=20)\n","            plt.xlabel(\"X1\",size=20)\n","            if drift:\n","                plt.title(\"MDS projection(Drift occurred): t = \"+str(self.t[-1]+1),size=20)\n","            else:\n","                plt.title(\"MDS projection: t = \"+str(self.t[-1]+1),size=20)\n","            plt.savefig(os.path.join(os.getcwd(),\"Projections\",\"MDS_\"+str(self.t[-1]+1)+\".pdf\"),format=\"pdf\")\n","        \n","    def save(self,folder,name):\n","        \n","        ## Delete most heavy data in the object and save it as pyhton object ##\n","        \n","        del self.kmeans\n","        del self.batches\n","        del self.all_batches\n","        with open(os.path.join(folder,name), 'wb') as output:\n","            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n","        \n","    def __str__(self):\n","        \n","        ## Return a string containing important information about the experiment procedure(parameters) ##\n","        \n","        n=40\n","        if self.jobs:\n","            if self.jobs < 0:\n","                return \"\\n\".join([\"Streaming Kmeans:\\n\",\"-\"*n,\"Data dimension = %s\" % self.batches[0].shape[1],\n","                          \"Number of clusters(k) = %s\" % self.k,\"Number of jobs(paralelization) = All\",\"First initialization: %s\" % self.first_init,\"-\"*n,\n","                          \"Forget parameter: \"+\"rho\"+\"= \"+str(self.rho[-2]),\"Threshold: \"+\"epsilon\"+\"= \"+str(self.thresh),\n","                          \"Maximum number of batches stored = %s\" % self.maxBatch,\"Number of processed batches = %s\" %(self.t[-1]+1)])\n","            else:\n","                return \"\\n\".join([\"Streaming Kmeans:\\n\",\"-\"*n,\"Data dimension = %s\" % self.batches[0].shape[1],\n","                          \"Number of clusters(k) = %s\" % self.k,\"Number of jobs(paralelization) = %s\" % self.jobs,\"First initialization: %s\" % self.first_init,\"-\"*n,\n","                          \"Forget parameter: \"+\"rho\"+\"= \"+str(self.rho[-2]),\"Threshold: \"+\"epsilon\"+\"= \"+str(self.thresh),\n","                          \"Maximum number of batches stored = %s\" % self.maxBatch,\"Number of processed batches = %s\" %(self.t[-1]+1)])\n","        else:\n","            return \"\\n\".join([\"Streaming Kmeans:\\n\",\"-\"*n,\"Data dimension = %s\" % self.batches[0].shape[1],\n","                          \"Number of clusters(k) = %s\" % self.k,\"Number of jobs(paralelization) = 1\",\"First initialization: %s\" % self.first_init,\"-\"*n,\n","                          \"Forget parameter: \"+\"rho\"+\"= \"+str(self.rho[-2]),\"Threshold: \"+\"epsilon\"+\"= \"+str(self.thresh),\n","                          \"Maximum number of batches stored = %s\" % self.maxBatch,\"Number of processed batches = %s\" %(self.t[-1]+1)])\n","    \n","    def summary(self):\n","        ## Print a summary ##\n","        print(self)\n","    \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"avNLTg1aDGC-"},"source":["### Boxplot"]},{"cell_type":"code","metadata":{"id":"SKla8eV1DGC_"},"source":["## Define functions in order to plot results ##\n","\n","def adapt_data(data=np.array([]),cd_per = 8): #Takes a data(stored over time), and modulates it to the concept drift period and can be plotted in boxplot format\n","    \n","    new_data = np.zeros(shape = (data.shape[1]//cd_per,cd_per,data.shape[0]))\n","    maximum = (data.shape[1]//cd_per)*cd_per\n","    for T in range(data.shape[0]):\n","        for t in range(cd_per):\n","            i = t\n","            index = list()\n","            while i<maximum:\n","                index.append(i)\n","                i+= cd_per\n","            new_data[:,(t)%cd_per,T] = data[T,index]\n","    return new_data   \n","\n","#The data to be plotted is stored in dictionaries. Each experiment has a dictionary, being the keys the names of hyperparameters and values its values. Keys are also related to the measurements, \n","#and the values are lists of the measures(initial error,elapsed time,...). xaxis is a dictionary with a single key, being the string to be plotted in the x axis, and the values to be stated as \n","#labels for this axis. Folder where the output pdf files should be saved. plot is a list of strings, which contains what measurements we want to plot. cm is the colormap. identity helps to \n","#differentiate the output apart from hyperparameter values on the title, save parameter works similarly but for the name of the saved file. scale accepts a dictionary with measures as keys\n","#and desired scales as values(default linear). Alpha determines the relative position of label ticks, should not be modified. ratio_ref is used if ratio is in plot list. cd_per \n","#is the period in which a concept is stable. rho_list is the list of forget parameters used, digit_r determines to which digit is round up when used as string. showfliers determines whether to\n","#show outliers or not. figsize determines the figure size, and epsilon is the value of epsilon during the experiments.\n","\n","def kmBoxplot(data=[],xaxis={},folder=None,plot=[None],cm=\"hsv\",identity=\"\",save = \"\",scale=dict(),alpha=1,\n","              ratio_ref = \"Minimize real error\",cd_per = 8,rho_list = [8,16], digit_r = 3, showfliers = True, figsize = (9,6),epsilon = 0.1):\n","    \n","    n_data = len(data)\n","    if n_data%2==0:\n","        relative=[i for i in range(-int(n_data/2),0)]\n","        relative.extend([i for i in range(1,int(n_data/2)+1)])\n","    else:\n","        relative=[i-n_data//2 for i in range(n_data)]\n","        \n","    ## Titles and y axis labels for each measurement ##\n","    \n","    title_dic = {\"surrogate_real\":\"Real and surrogate error difference histogram\",\"time\":\"Elapsed time\",\"error_real_conver\":\"Real error in convergence\",\"error_real_init\":\"Real initialization error\",\"error_init\":\"Initialization error comparison\",\"ratio\":\"Error ratio\",\"error_conver\":\"Converged error comparison\",\"n_iter\":\"Number of iterations comparison\",\"n_dis\":\"Computed distances comparison\",\"n_dis_lloyd\":\"Computed distances during Lloyd\",\"n_dis_init\": \"Computed distances during initialization\"}\n","    ylab_dic = {\"surrogate_real\":\"Density\",\"time\":\"Elapsed time(s)\",\"error_real_conver\":\"Normalized error\",\"error_real_init\":\"Normalized error\",\"error_init\": \"Normalized error\",\"ratio\":\"Error ratio\",\"error_conver\":\"Normalized error\",\"n_iter\":\"N iter\",\"n_dis\":\"N distances\",\"n_dis_lloyd\":\"N distances\",\"n_dis_init\":\"N distances\"}\n","    \n","    ## Run through all requested plots ##\n","    \n","    for pl in plot:\n","        \n","        if pl in scale.keys():\n","            sc = scale[pl]\n","        else:\n","            sc = \"linear\"\n","        if pl == \"surrogate_real\" or pl == \"ratio\":\n","            continue\n","        \n","        for T in range(len(rho_list)):\n","            \n","            r = rho_list[T]\n","                \n","            fig = plt.figure(1, figsize=figsize)\n","            ax = fig.add_subplot(111) #Create an axes instance\n","            cmap = matplotlib.cm.get_cmap(cm)  #Create a colormap instance\n","\n","            b = [None for _ in range(len(data))] #Boxplot plots list\n","\n","            for i in range(len(data)):\n","                col = cmap(i/(len(data))) #Define color for each method\n","                pos1 = np.array([value for value in xaxis.values()][0]) #Compute positions for each boxplot\n","                pos = alpha*len(data)*pos1\n","                for j in range(len(pos)):\n","                    pos[j]=pos[j]+relative[i]\n","                box = adapt_data(data[i][pl],cd_per)[:,:,T]\n","\n","                ## Plot each boxplot ##\n","                b[i] = ax.boxplot(box,patch_artist=True,\n","                            positions=pos,boxprops=dict(facecolor=col,color=\"black\"),\n","                            whiskerprops=dict(color=col,linestyle=\"--\"),capprops=dict(color=\"black\"),medianprops=dict(color=\"black\"),\n","                            flierprops=dict(marker=\"o\",color=col,alpha=1),manage_ticks = False, showfliers = showfliers)\n","                for flier in b[i][\"fliers\"]:\n","                    flier.set_markerfacecolor(col)\n","                    \n","            ## Set axis parameters and text ##\n","            ax.set_xticks(alpha*len(data)*pos1)\n","            ax.set_xticklabels([str(x) for x in [value for value in xaxis.values()][0]])\n","            ax.get_yaxis().tick_left()\n","            ax.legend([box[\"boxes\"][0] for box in b], [ dat[\"name\"] for dat in data], loc='upper right')\n","            ax.set_yscale(sc)\n","            pos = [value for value in xaxis.values()][0]\n","            if identity!=\"\":\n","                ax.set_title(title_dic[pl]+\": \"+identity,fontsize=\"xx-large\")\n","            else:\n","                ax.set_title(title_dic[pl],fontsize=\"xx-large\")\n","            ax.set_ylabel(ylab_dic[pl])\n","            ax.set_xlabel([value for value in xaxis.keys()][0])\n","            \n","            ## Draw vertical lines to separate each index ##\n","            for i in range(len(pos1)-1):\n","                ax.axvline(x = alpha*len(data)*(pos1[i]+pos1[i+1])/2,linestyle = \"-.\",color = \"grey\",linewidth = 1)\n","\n","            ## Save plots as pdf ##\n","            if folder:\n","                if save:\n","                    fig.savefig(os.path.join(folder,pl,pl+save+\".pdf\"),format=\"pdf\")\n","                else:\n","                    fig.savefig(os.path.join(folder,pl,pl+identity+\".pdf\"),format=\"pdf\")\n","            else:\n","                fig.savefig(os.path.join(parent(path),\"Results\",pl,pl+identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\", \"+\"epsilon\"+\" = \"+str(epsilon)+\".pdf\"),format=\"pdf\")\n","            fig.clf()    \n","    \n","    if \"surrogate_real\" in plot: #This one is computes relative differences between surrogate and SKM error\n","        pl = \"surrogate_real\"\n","        for T in range(len(rho_list)):\n","            r = rho_list[T]\n","            data_list = np.array([])\n","            for i in range(len(data)):\n","                Data = abs(data[i][\"error_real_conver\"]-data[i][\"error_conver\"])/data[i][\"error_real_conver\"]\n","                Data = Data[T,:]\n","                data_list = np.concatenate((data_list,Data),axis = 0)\n","            if len(data_list)==0:\n","                continue\n","            fig = plt.figure(1, figsize=figsize)\n","            ax = fig.add_subplot(111)\n","            ax.hist(np.log(data_list)/np.log(10),color = \"teal\",bins = 100)\n","            ax.set_title(title_dic[pl]+identity,fontsize=\"xx-large\")\n","            ax.set_ylabel(ylab_dic[pl])\n","            ax.set_xlabel(r\"$\\frac{|E_T-E_{\\rho}|}{E_T}$\",fontsize=\"x-large\")\n","            if folder:\n","                fig.savefig(os.path.join(folder,pl,pl+identity+\".pdf\"),format=\"pdf\")\n","            else:\n","                fig.savefig(os.path.join(parent(path),\"Results\",pl,pl+identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\".pdf\"),format=\"pdf\")\n","            fig.clf()\n","        \n","    if \"ratio\" in plot: #Plots measurements in ratios compared to the one determined by ratio_ref\n","    \n","        pl = \"ratio\"\n","        k = np.where(np.array([ dat[\"name\"] for dat in data]) == ratio_ref)[0][0]\n","        ref = data.pop(k)\n","        \n","        for error in [\"error_conver\",\"error_init\",\"error_real_conver\",\"error_real_init\"]:\n","            for T in range(len(rho_list)):\n","                \n","                r = rho_list[T]\n","                fig = plt.figure(1, figsize=figsize)\n","\n","                ## Create an axes instance ##\n","                ax = fig.add_subplot(111)\n","                cmap = matplotlib.cm.get_cmap(cm)\n","\n","                b = [None for _ in range(len(data))]\n","                \n","                for i in range(len(data)):\n","                    col = cmap(i/(len(data)))\n","                    pos1 = np.array([value for value in xaxis.values()][0])\n","                    pos = alpha*len(data)*pos1\n","                    for j in range(len(pos)):\n","                        pos[j]=pos[j]+relative[i]\n","                    box = adapt_data((data[i][error]-ref[error])/ref[error],cd_per)[:,:,T]\n","                    b[i] = ax.boxplot(box,patch_artist=True,\n","                                positions=pos,boxprops=dict(facecolor=col,color=\"black\"),\n","                                whiskerprops=dict(color=col,linestyle=\"--\"),capprops=dict(color=\"black\"),medianprops=dict(color=\"black\"),\n","                                flierprops=dict(marker=\"o\",color=col,alpha=1),manage_ticks = False, showfliers = showfliers)\n","                    for flier in b[i][\"fliers\"]:\n","                        flier.set_markerfacecolor(col)\n","                ax.set_xticks(alpha*len(data)*pos1)\n","                ax.set_xticklabels([str(x) for x in [value for value in xaxis.values()][0]])\n","                ax.get_yaxis().tick_left()\n","                ax.legend([box[\"boxes\"][0] for box in b], [ dat[\"name\"] for dat in data if dat[\"name\"]!=ratio_ref], loc='lower right')\n","                ax.set_yscale(scale)\n","                for xv in alpha*len(data)*pos1:\n","\n","                    ax.axvline(x = xv,linestyle = \"-.\",color = \"grey\",linewidth = 1)\n","\n","                if identity!=\"\":\n","                    ax.set_title(title_dic[pl]+\"(\"+error+\"): \"+identity,fontsize=\"xx-large\")\n","                else:\n","                    ax.set_title(title_dic[pl]+\"(\"+error+\")\",fontsize=\"xx-large\")\n","                ax.set_ylabel(ylab_dic[pl])\n","                ax.set_xlabel([value for value in xaxis.keys()][0])\n","                if folder:\n","                    fig.savefig(os.path.join(folder,pl,pl+error+identity+\".pdf\"),format=\"pdf\")\n","                else:\n","                    fig.savefig(os.path.join(parent(path),\"Results\",pl,pl+error+identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\".pdf\"),format=\"pdf\")\n","                fig.clf()      \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q92txEDkDGC_"},"source":["## This function plots the results of the experiment where the surrogate is compared to the SKM error ##\n","\n","#Data contains the measured differences between both error functions, up and low contain theoretical upper and lower bounds. \n","#Every other parameter work similar to the previous function, plus there is another parameter N which states the batch size of the experiment.\n","\n","def SurrogateBoxplot(data=[],up=[],low=[],xaxis={},folder=None,r = 0.1,epsilon=0.1,cm=\"hsv\",identity=\"\",scale=\"linear\",alpha=1,\n","                     showfliers = True, figsize = (16,9), N = 10,delta_list = []):\n","                \n","    identity = \"Simulated Data\"\n","    fig = plt.figure(1, figsize=figsize)\n","\n","    ## Create an axes instance ##\n","    ax = fig.add_subplot(111)\n","    cmap = matplotlib.cm.get_cmap(cm)\n","    col = cmap(0.4)\n","    pos1 = np.array([value for value in xaxis.values()][0])\n","    s = 10\n","    ## Plot boxplots ##\n","    b = ax.boxplot(data,patch_artist=True,\n","                positions=pos1,boxprops=dict(facecolor=col,color=\"black\"),\n","                whiskerprops=dict(color=col,linestyle=\"--\"),capprops=dict(color=\"black\"),medianprops=dict(color=\"black\"),\n","                flierprops=dict(marker=\"o\",color=col,alpha=1),manage_ticks = False, showfliers = showfliers)\n","    \n","    for flier in b[\"fliers\"]:\n","        flier.set_markerfacecolor(col)\n","    ax.tick_params(axis='both', which='major', labelsize=s)\n","    x = np.linspace(pos1[0],pos1[-1],100)\n","    gauss = [0 for i in range(up.shape[0])]\n","    gauss_label = [Patch(facecolor = \"b\",alpha = (i+1)/(up.shape[0]+1),label = \"%s\"%(int(100*(1-delta_list[i])))+\"\\% confidence interval\") for i in range(up.shape[0])]\n","    ## Interpolate lines for upper and lower bounds and fill the space between them ##\n","    for i in range(up.shape[0]):\n","        upper = up[i,:]\n","        alpha = (i+1)/(up.shape[0]+1)\n","        interpolate = scipy.interpolate.make_interp_spline(pos1, upper)\n","        interpolate_upper = interpolate(x)\n","        l1 = ax.plot(x,interpolate_upper,alpha=alpha, color='b')\n","        lower = low[i,:]\n","        interpolate = scipy.interpolate.make_interp_spline(pos1, lower)\n","        interpolate_lower = interpolate(x)\n","        l2 = ax.plot(x,interpolate_lower,alpha=alpha, color='b')\n","        gauss[i] = plt.fill(np.concatenate([x, x[::-1]]),\n","             np.concatenate([interpolate_upper,\n","                            (interpolate_lower)[::-1]]),\n","             alpha=alpha, fc='b', ec='None')\n","    \n","    ## Axis setup ##\n","    ax.set_xticks(pos1)\n","    ax.set_xticklabels([str(x) for x in [value for value in xaxis.values()][0]],fontsize=s)\n","    ax.get_yaxis().tick_left()\n","    legend_elements = [Patch(facecolor = col,label = \"Observations\")]\n","    legend_elements.extend(gauss_label)\n","    s = 60\n","    ## Use tex fonts ##\n","    plt.rcParams.update({\n","    \"text.usetex\": True,\n","    \"font.weight\":\"light\"\n","})\n","    font = {\n","    \"fontweight\":\"light\",\n","    \"fontsize\":s\n","}\n","    ax.legend(handles=legend_elements,loc='upper right',fontsize=35)\n","    ax.tick_params(axis='both', which='major', labelsize=25)\n","    ax.set_yscale(scale)\n","    ax.set_xlabel(r\"Number of batches since last drift, $T$\",fontdict=font)\n","    ax.set_ylabel(r\"$E_*-E_{\\rho}$\",fontdict=font)\n","    ax.set_ylim(-1500,7500) #Set limits of y axis, same limits for each every experiment for comparability\n","    if identity!=\"\":\n","        ax.set_title(r\"$\\rho$\"+\" = \"+str(round(r,3))+\", \"+r\"$N$\"+\" = \"+str(N),fontdict=font)\n","    else:\n","        ax.set_title(\"rho\"+\" = \"+str(round(r,3))+\", \"+\"epsilon\"+\" = \"+str(epsilon)+\", N = \"+str(N),fontsize=s)\n","    \n","    ## Save figure ##\n","    if folder:\n","        fig.savefig(os.path.join(folder,identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\", \"+\"epsilon\"+\" = \"+str(epsilon)+\", N = \"+str(N)+\".pdf\"),format=\"pdf\")\n","    else:\n","        fig.savefig(os.path.join(parent(path),\"Surrogate\",identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\", \"+\"epsilon\"+\" = \"+str(epsilon)+\", N = \"+str(N)+\".pdf\"),format=\"pdf\")\n","    fig.clf() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lY3iNykDGDA"},"source":["### Experiment Surrogate vs SKM error\n"]},{"cell_type":"code","metadata":{"id":"nxb3sieQDGDA"},"source":["os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Surrogate\")))\n","names = [\"SurrogateData05\",\"SurrogateData01\"]\n","use = {name:False for name in names}\n","\n","if SurrogateExperiment: #If SurrogateExperiment is False then do not conduct any experiments\n","    use[names[0]] =True\n","    use[names[1]] =True\n","\n","eps_dict = {\"SurrogateData01\":0.1,\"SurrogateData05\":0.5,\"SurrogateData1\":1}\n","rho_list = [1]#[1./2,1./4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7gpfoTrDGDA"},"source":["## Run experiment of the adecuacy of the surrogate function ##\n","\n","log_file = open(os.path.join(parent(path),\"ExperimentLog\"),\"w\") #Log file where the process of the algorithm can be consulted\n","N_list = [1000,4000] #List of number of batches for different experiments\n","delta_list = [1-0.95,1-0.68] #Determine delta value to compute theoretical bounds\n","repetitions = round(2/min(delta_list)) #How many times repeat the experiment\n","n_batches = 20 #Number of batches from the initial concept\n","D = 0.01 #Threshold\n","\n","folder = os.path.join(parent(path),\"Python Objects\") #Where to save the data\n","\n","for N in N_list:\n","    for name in names:\n","        if use[name]:\n","            data_name = name\n","            file = os.path.join(parent(path),\"Python Objects\",data_name)\n","            with open(os.path.join(file), 'rb') as input:    \n","                (concept_size,data)=pickle.load(input)\n","            dataset1 = data[0:concept_size,:]\n","            dataset2 = data[concept_size:2*concept_size,:]\n","            center = Mean(dataset1,ax = 0) #The single centroid is the center of mass of the first concept\n","            batch_size = N\n","            E = Mean(np.linalg.norm(dataset1-center,axis = 1)**2,ax = 0) #The KM error of the first concept\n","            dataset = np.concatenate((dataset1,dataset2),axis = 0)\n","            maximum = np.linalg.norm(dataset-center,axis = 1).max()**2\n","            minimum = np.linalg.norm(dataset-center,axis = 1).min()**2\n","            b = maximum-minimum\n","            epsilon = eps_dict[name]\n","            for r in rho_list:\n","                rho = (D/epsilon)**(r/(n_batches))\n","                print(rho)\n","                print_and_log(log_file,\"Rho = %s\\n\"%rho)\n","                generated = False\n","                for delta in delta_list:\n","                    upper_bound_list = []\n","                    lower_bound_list = []\n","                    for t in range(n_batches):\n","                        e = b*np.sqrt(((1-rho)/(1+rho)+(2*rho**(t+1)-1)/(t+1))*np.log(2/delta)/(2*N))\n","                        d = rho**(t+1)*epsilon*E\n","                        upper_bound_list.append(e+d) #Theoretical upper bound\n","                        lower_bound_list.append(d-e) #Theoretical lower bound\n","                    if generated:\n","                        upper_bound = np.concatenate((upper_bound,np.array([upper_bound_list])))\n","                        lower_bound = np.concatenate((lower_bound,np.array([lower_bound_list])))\n","                    else:\n","                        upper_bound = np.array([upper_bound_list])\n","                        lower_bound = np.array([lower_bound_list])\n","                        generated = True\n","\n","                generated = False\n","\n","                index = np.array([i for i in range(dataset1.shape[0])])\n","                for i in range(repetitions):\n","                    print(\"Repetition %s\"%i)\n","                    print_and_log(log_file,\"Repetition %s\"%i)\n","                    batches = []\n","                    real_batches = []\n","                    dif_list = []\n","                    for i in range(2*n_batches):\n","                        random = np.random.choice(index,size = batch_size)\n","                        batches.append(dataset1[random,:])\n","                    for t in range(n_batches):\n","                        random = np.random.choice(index,size = batch_size)\n","                        new = dataset2[random,:]\n","                        real_batches.append(new)\n","                        batches.append(new)\n","                        Et = inertia(np.vstack(real_batches),centers = np.array([center])) #Compute SKM error\n","                        Ep = inertia(np.vstack(batches),centers = np.array([center]),weight = np.array([rho**(-i-1) for i in range(-len(batches),0,+1) for _ in range(batches[i].shape[0])])) #Compute surrogate eror \n","                        dif = Et-Ep\n","                        dif_list.append(dif)\n","                    if not generated:\n","                        generated = True\n","                        data = np.array([dif_list])\n","                    else:\n","                        data = np.concatenate((data,np.array([dif_list])))\n","\n","                data = np.transpose(np.sort(np.transpose(data))[:,1:-1])\n","                xaxis = {r\"$T$\":[i+1 for i in range(n_batches)]}\n","\n","                ID = name+\" \"+\"rho\"+\" = \"+str(rho)+\" \"+\"epsilon\"+\"=\"+str(epsilon)+\"N=\"+str(N)\n","                with open(os.path.join(folder,ID), 'wb') as output:\n","                    pickle.dump((data,upper_bound,lower_bound,xaxis), output, pickle.HIGHEST_PROTOCOL) #Save data as python object\n","\n","                SurrogateBoxplot(data=data,up=upper_bound,low=lower_bound,xaxis=xaxis,r = rho,epsilon=epsilon,identity=name, N = N,delta_list = delta_list) #Plot the results\n","            \n","log_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjU6g6obDGDB"},"source":["## If the experiments were already conducted and data stored, then one can replot the data, making possible to change some plot parameters ##\n","\n","folder = os.path.join(parent(path),\"Python Objects\")\n","replot = True\n","log_file = open(os.path.join(parent(path),\"ExperimentLog\"),\"w\")\n","N = 10000\n","N_list = [1000,4000]\n","delta_list = [1-0.95,1-0.68]\n","repetitions = round(2/min(delta_list))\n","n_batches = 20\n","D = 0.01\n","rho_list = [1,1./2]\n","xaxis = {r\"$T$\":[i+1 for i in range(n_batches)]}\n","\n","if replot:\n","    for N in N_list:\n","        for name in names:\n","            if use[name]:\n","                \n","                epsilon = eps_dict[name]\n","                data_name = name       \n","                for r in rho_list:\n","                    rho = (D/epsilon)**(1./(n_batches*r))\n","                    print(rho)\n","                    ID = name+\" \"+\"rho\"+\" = \"+str(rho)+\" \"+\"epsilon\"+\"=\"+str(epsilon)+\"N=\"+str(N)\n","\n","                    with open(os.path.join(folder,ID), 'rb') as input:\n","                        (data,upper_bound,lower_bound,xaxis) = pickle.load(input)\n","                    \n","                    SurrogateBoxplot(data=data,up=upper_bound,figsize = (16,10),low=lower_bound,xaxis=xaxis,r = rho,epsilon=epsilon,identity=name, N = N,delta_list = delta_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HhoaPqtjDGDC"},"source":["### Streaming K-means Experiments"]},{"cell_type":"markdown","metadata":{"id":"fCwTahIQDGDC"},"source":["### Define experiment parameters"]},{"cell_type":"code","metadata":{"id":"ffvjAxeEDGDC"},"source":["## Here parameters are defined for the SKM experiments ##\n","\n","#Name for each method, there are many that are not used, if they are removed check indexes in the following (dict[names[index]]) dictionaries.\n","#Their names were changed for the final paper, check their definitive names on the next if statement.\n","names = [\"Minimize SKM error\",\"Use previous c\",\"New batch initialization\",\"Forget centers(Forgy)\",\"MB 100\",\"MB 1000\",\"kmc2\",\"Use previous\"\n","         ,\"SSKMI(H)\",\"Fix centers\",\"Nearest points\",\"WSKMI\",\"SSKMI\",\"Use previous centers\"]\n","\n","use = {name:False for name in names}\n","\n","if EficiencyExperiment: #If EficiencyExperiment is False, then no experiments are run\n","    \n","    use[names[0]] = True ##Minimize real error (PSKM)\n","    use[names[13]] = True ##Use previous centers (PC)\n","    use[names[2]] = True ##Initialize based on new batch (CC)\n","    use[names[8]] = True ## Simplified SKMI (HI)\n","    use[names[11]] = True ##Kmeans on centroids (WI)\n","\n","file_names = {names[i-1]:\"skm\"+str(i) for i in range(1,len(names)+1)}\n","\n","## Default values ##\n","d_thresh = 0.01\n","d_maxBatch = None\n","d_method = None\n","d_first_init = \"k-means++\"\n","d_chain_length = 200\n","d_afkmc2 = True\n","d_jobs = None\n","d_n_init = 1\n","d_mini_batch_size = None\n","d_hungaro = True\n","\n","## Specific values for each method ##\n","\n","hungaro_dict = {\"skm13\":False}\n","\n","eps_dict = {\"SimulatedData01\":0.1,\"SimulatedData05\":0.5}\n","\n","mini_dict = {\"skm5\":100,\"skm6\":1000}\n","first_init_dict = {\"skm4\":\"random\",\"skm7\":\"kmc2\"}\n","method_dict = {\"skm1\":\"minimize_real\",\"skm14\":\"prev_centers\",\"skm8\":\"prev\",\"skm13\":\"weights\",\"skm9\":\"weights\",\"skm10\":\"fix_new_centers\",\"skm11\":\"nearest_points\",\"skm12\":\"centroid_kmeans\"}\n","\n","d_consider_concept_drift = False\n","d_forget = False\n","\n","cd_dict = {\"skm1\":False}\n","forget_dict = {\"skm3\":True,\"skm4\":True,\"skm5\":True,\"skm6\":True,\"skm7\":True}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z8psdqEzDGDC"},"source":["### Execute experiment"]},{"cell_type":"code","metadata":{"id":"eknlSJBTDGDD"},"source":["## Run main experiments ##\n","\n","log_file = open(os.path.join(parent(path),\"ExperimentLog\"),\"w\") #Log file\n","m_list = np.array([1,2]) #List of m values\n","k_list = [5,10,25,50] #List K values\n","k_from_list = True #Whether to use K from list or from the dictionary with optimal K for each dataset\n","e_list = [2] #List of concept drift sizes\n","e_dict = {0.5:\"05\",1:\"1\",2:\"2\"} #Dictonary relating value with string for file saving\n","cd_list = [10] #List of periods where a concept is stable\n","batch_size_list = [500] #Batch size list, if more than one batch size need to be compared\n","data_name_list = [\"Urban\",\"SUSY\",\"Pulsar\",\"Epilepsia\",\"Google\",\"Frogs\",\"Gesture\",\"Gas\"] #List of datasets to use\n","centers = dict() #Here C^* are stored from UPC, in order to set the same centroids for other methods\n","\n","projection_plot = [\"\"]#[\"mds\",\"pca\"] #If wanted to plot data projection\n","experiment_start = time.time() #Measure elapsed time\n","\n","if EficiencyExperiment:\n","    for data_name in data_name_list:\n","        print(\"Data:%s\"%(data_name))\n","        for epsilon in e_list:\n","            print(\"Epsilon=%s\"%(epsilon))\n","            for b_size in batch_size_list:\n","                print(\"Batch size=%s\"%b_size)\n","                print_and_log(log_file,\"Batch size = %s\\n\"%b_size)\n","                prin =\"\\ \\ \\ \\ \"\n","                for K in k_list:\n","                    if not k_from_list:\n","                        K = k_dict[data_name]\n","                    print(\"K=%s\"%K)\n","                    print_and_log(log_file,prin+\"Number of clusters: K = %s\\n\"%K)\n","                    for cd_per in cd_list:\n","                        print_and_log(log_file,prin+\"Concept drift period = %s\\n\"%cd_per)\n","                        prin+=\"\\ \\ \\ \\ \"\n","                        for CD in [True]:#[True,False]:\n","\n","                                start = time.time() #Elapsed time for each experiment in each dataset\n","\n","                                for m in m_list:\n","                                    \n","                                    is_generated = dict()\n","                                    for name in names:\n","                                        if use[name]:\n","                                            is_generated[name] = False\n","                                            \n","                                    ## Define some dictionaries that store measurements ##\n","                                    skm = dict()\n","                                    elapsed_time = dict()\n","                                    error_conver = dict()\n","                                    error_init = dict()\n","                                    error_real_init = dict()\n","                                    error_real_conver = dict()\n","                                    n_iter = dict()\n","                                    n_dis = dict()\n","                                    n_dis_init = dict()\n","                                    n_dis_lloyd = dict()\n","                                    thresh = d_thresh\n","                                    dat = data_name\n","                                    r = float((thresh/epsilon)**(m/cd_per))\n","                                    print(\"Rho=%s\"%r)\n","                                    print_and_log(log_file,prin+\"rho = %s\"%r)\n","                                    tmax_thresh = 1-0.9999\n","                                    tmax = math.ceil(math.log(tmax_thresh)/math.log(r))\n","                                    print(\"Tmax=%s\"%tmax)\n","                                    is_file = True\n","                                    stream = StreamCD(file = os.path.join(parent(path),\"Python Objects\",data_name+e_dict[epsilon]+\"k=\"+str(K)),batch_size = b_size,cd_period = cd_per,\n","                                                      first_batches=os.path.join(parent(path),\"Datasets\",direc_dict[data_name]),tmax = tmax, is_file = is_file,class_index = index_dict[data_name])\n","                                    (init_batch,drift) = next(stream)\n","\n","                                    ## Set parameters for each method for the first batch ##\n","                                    for name in names:\n","                                        if use[name]:\n","                                            code = file_names[name]\n","\n","                                            if code in mini_dict.keys():\n","                                                mini_batch_size = mini_dict[code]\n","                                            else:\n","                                                mini_batch_size = d_mini_batch_size\n","\n","                                            if code in first_init_dict.keys():\n","                                                first_init = first_init_dict[code]\n","                                            else:\n","                                                first_init = d_first_init\n","\n","                                            if code in method_dict.keys():\n","                                                method = method_dict[code]\n","                                            else:\n","                                                method = d_method\n","\n","                                            if code in cd_dict.keys():\n","                                                consider_concept_drift = cd_dict[code]\n","                                            else:\n","                                                consider_concept_drift = d_consider_concept_drift\n","\n","                                            if code in forget_dict.keys():\n","                                                forget = forget_dict[code]\n","                                            else:\n","                                                forget = d_forget\n","                                            \n","                                            if code in hungaro_dict.keys():\n","                                                hungaro = hungaro_dict[code]\n","                                            else:\n","                                                hungaro = d_hungaro\n","                                            maxBatch = d_maxBatch\n","                                            chain_length = d_chain_length\n","                                            afkmc2 = d_afkmc2 \n","                                            jobs = d_jobs\n","                                            n_init = d_n_init \n","                                            mini_batch_size = d_mini_batch_size\n","\n","                                            ## Set the KM problem ##\n","                                            if name == \"Use previous centers\":\n","                                                skm[name] = SkMeans(initial_batch =init_batch,k = K,rho =r,thresh = thresh,method = method, first_init = first_init,chain_length = chain_length,\n","                                                      afkmc2 = afkmc2,jobs =jobs,n_init =n_init,mini_batch_size = mini_batch_size,plot = projection_plot)\n","                                                prev_centers = skm[name].centers[-1]\n","                                            else:\n","                                                skm[name] = SkMeans(initial_batch =init_batch,k = K,rho =r,thresh = thresh,method = method, first_init = first_init,chain_length = chain_length,\n","                                                      afkmc2 = afkmc2,jobs =jobs,n_init =n_init,mini_batch_size = mini_batch_size,plot = projection_plot)\n","\n","                                    i = 0\n","                                    cd_index = dict()\n","                                    for name in names:\n","                                            if use[name]:\n","                                                cd_index[name] = 2\n","                                                \n","                                    ## Go through the stream and set parameters for each method ##\n","                                    for (batch,drift) in stream:\n","                                        for name in names:\n","                                            \n","                                            if use[name]:\n","                                                code = file_names[name]\n","\n","                                                if code in mini_dict.keys():\n","                                                    mini_batch_size = mini_dict[code]\n","                                                else:\n","                                                    mini_batch_size = d_mini_batch_size\n","\n","                                                if code in first_init_dict.keys():\n","                                                    first_init = first_init_dict[code]\n","                                                else:\n","                                                    first_init = d_first_init\n","\n","                                                if code in method_dict.keys():\n","                                                    method = method_dict[code]\n","                                                else:\n","                                                    method = d_method\n","\n","                                                if code in cd_dict.keys():\n","                                                    consider_concept_drift = cd_dict[code]\n","                                                else:\n","                                                    consider_concept_drift = d_consider_concept_drift\n","\n","                                                if code in forget_dict.keys():\n","                                                    forget = forget_dict[code]\n","                                                else:\n","                                                    forget = d_forget\n","                                                maxBatch = d_maxBatch\n","                                                chain_length = d_chain_length\n","                                                afkmc2 = d_afkmc2 \n","                                                jobs = d_jobs\n","                                                n_init = d_n_init \n","                                                mini_batch_size = d_mini_batch_size\n","\n","                                                if drift: #If a drift occurs reset the index where the concept drift occurred\n","                                                    cd_index[name] = 1\n","                                                    do = False\n","                                                    if do:\n","                                                        check_optimal_p(skm = skm, batch = batch, hungaro = hungaro, r = r, data_name = data_name, N = 30, scale = \"linear\")\n","                                                        break\n","                                                do_plot = False\n","                                                if cd_per-cd_index[name]%cd_per<=2:\n","                                                    do_plot = True\n","                                                elif cd_index[name]%cd_per<=2:\n","                                                    do_plot = True\n","                                                i+=1\n","                                                \n","                                                ## Set KM problem ##\n","                                                skm[name].Kmeans(batch,forget_centers=forget,concept_drift = (drift and consider_concept_drift),cd_index = cd_index[name],\n","                                                                 plot = projection_plot,do_plot = do_plot,drift = drift,hungaro = hungaro, prev_centers = prev_centers)\n","                                                cd_index[name]+=1\n","                                                \n","                                                ## If UPC save C^* ##\n","                                                if name == \"Use previous centers\":\n","                                                    prev_centers = skm[name].centers[-1]\n","                                        \n","                                    ## Save results ##\n","                                    for name in names:\n","                                        if use[name]:\n","                                            if is_generated[name]:\n","                                                elapsed_time[name] = np.concatenate((elapsed_time[name],[skm[name].time[tmax+cd_per:]]),axis=0)\n","                                                error_conver[name] = np.concatenate((error_conver[name],[skm[name].error_conver[tmax+cd_per:]]),axis=0)\n","                                                error_init[name] = np.concatenate((error_init[name],[skm[name].error_init[tmax+cd_per:]]),axis=0)\n","                                                error_real_init[name] = np.concatenate((error_real_init[name],[skm[name].error_real_init[tmax+cd_per:]]),axis=0)\n","                                                error_real_conver[name] = np.concatenate((error_real_conver[name],[skm[name].error_real_conver[tmax+cd_per:]]),axis=0)\n","                                                n_iter[name] = np.concatenate((n_iter[name],[skm[name].lloyd_iter[tmax+cd_per:]]),axis=0)\n","                                                n_dis[name] = np.concatenate((n_dis[name],[skm[name].dis[tmax+cd_per:]]),axis=0)\n","                                                n_dis_init[name] = np.concatenate((n_dis_init[name],[skm[name].dis_init[tmax+cd_per:]]),axis=0)\n","                                                n_dis_lloyd[name] = np.concatenate((n_dis_lloyd[name],[skm[name].dis_lloyd[tmax+cd_per:]]),axis=0)\n","\n","                                            else:\n","                                                elapsed_time[name] = np.array([skm[name].time[tmax+cd_per:]])\n","                                                error_conver[name] = np.array([skm[name].error_conver[tmax+cd_per:]])\n","                                                error_init[name] = np.array([skm[name].error_init[tmax+cd_per:]])\n","                                                error_real_init[name] = np.array([skm[name].error_real_init[tmax+cd_per:]])\n","                                                error_real_conver[name] = np.array([skm[name].error_real_conver[tmax+cd_per:]])\n","                                                n_iter[name] = np.array([skm[name].lloyd_iter[tmax+cd_per:]])\n","                                                n_dis[name] = np.array([skm[name].dis[tmax+cd_per:]])\n","                                                n_dis_init[name] = np.array([skm[name].dis_init[tmax+cd_per:]])     \n","                                                n_dis_lloyd[name] = np.array([skm[name].dis_lloyd[tmax+cd_per:]])\n","                                                is_generated[name] = True\n","\n","                                    for name in names:\n","                                        if use[name]:\n","                                            d = dict()\n","                                            d[\"name\"] = name\n","                                            d[\"r\"] = r\n","                                            d[\"time\"] = elapsed_time[name]\n","                                            d[\"error_conver\"] = error_conver[name]\n","                                            d[\"error_init\"] = error_init[name]\n","                                            d[\"error_real_init\"] = error_real_init[name]\n","                                            d[\"error_real_conver\"] = error_real_conver[name]\n","                                            d[\"n_iter\"] = n_iter[name]\n","                                            d[\"n_dis\"] = n_dis[name]\n","                                            d[\"n_dis_init\"] = n_dis_init[name]\n","                                            d[\"n_dis_lloyd\"] = n_dis_lloyd[name]\n","                                            folder = os.path.join(parent(path),\"Python Objects\")\n","                                            if CD:\n","                                                ID = name+\"K\"+str(K)+\"exp90mass_cd\"+data_name+\"epsilon\"+str(epsilon)+\"rho\"+str(round(r,3))\n","                                            else:\n","                                                ID = name+\"K\"+str(K)+\"exp90mass\"+data_name+\"epsilon\"+str(epsilon)\n","                                            with open(os.path.join(folder,ID), 'wb') as output:\n","                                                pickle.dump(d, output, pickle.HIGHEST_PROTOCOL)\n","                                            print_and_log(log_file,prin+\"Saved results in %s\"%(os.path.join(folder,ID)))\n","                                            print_and_log(log_file,prin+\"Time(h) for %s with code %s = %s h\\n\"%(name,code,(time.time()-start)/3600))\n","    ## Print elapsed total time ##\n","    print_and_log(log_file,\"Experiment runtime(h) = %s h\"%((time.time()-experiment_start)/3600))\n","    print(\"Done\")\n","log_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5-2YwyIDGDD"},"source":["### Boxplots\n"]},{"cell_type":"code","metadata":{"id":"-WArdka-DGDK"},"source":["## Plot results ##\n","\n","#Uncomment the desrired measurements to be plotted\n","plot_list = [\"time\"]#[\"error_real_init\",\"error_conver\",\"n_iter\",\"n_dis\",\"error_init\",\"error_real_conver\"]#[\"surrogate_real\",\"error_conver\",\"n_iter\",\"n_dis\",\"error_init\",\"n_dis_init\",\"n_dis_lloyd\",\"error_real_init\",\"error_real_conver\",\"time\",\"ratio\"]#,\"ratio\"\n","#Make directories where figures are saved\n","os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\")))\n","for data_name in data_name_list:\n","    os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\",data_name)))\n","    for name in plot_list:\n","        os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\",data_name,name)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQvt2O-vDGDM"},"source":["## Names of methods ##\n","names = [\"Minimize SKM error\",\"Use previous centers\",\"New batch initialization\",\"Forget centers(Forgy)\",\"MB 100\",\"MB 1000\",\"kmc2\",\"Use previous\"\n","         ,\"SSKMI(H)\",\"SSKMI\",\"Fix centers\",\"Nearest points\",\"WSKMI\"]\n","use = {name:False for name in names}\n","\n","## Specify which methods to be plotted, FSKM with different initialization and/or PSKM ##\n","if EficiencyExperiment:\n","    #use[names[0]] = True ##Minimize real error (PSKM)\n","    use[names[1]] = True ##Use previous centers (PC)\n","    use[names[2]] = True ##Initialize based on new batch (CC)\n","    use[names[8]] = True ##Weights(p) (HI)\n","    use[names[12]] = True ##Kmeans on centroids (WI)\n","\n","## This function simply computes how many methods will be plotted, used later for plotting porpuses ##\n","def how_many(use):\n","    k = 0\n","    for key in use.keys():\n","        if use[key]:\n","            k+=1\n","    return k"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_J3yohGkDGDM"},"source":["## Here boxplots are plotted using matplotlib ##\n","\n","folder = os.path.join(parent(path),\"Python Objects\")\n","scale_dict = {\"n_init\":\"linear\"}\n","for data_name in data_name_list:\n","    for epsilon in e_list:\n","        for K in k_list:\n","            if not k_from_list:\n","                K = k_dict[data_name]\n","            for b_size in batch_size_list:\n","                for cd_per in cd_list:\n","                    xaxis = {\"t\":list(range(cd_per))}\n","                    for CD in [True]:\n","                        for m in m_list:\n","                            thresh = d_thresh\n","                            r = float((thresh/epsilon)**(m/cd_per))\n","                            print(r)\n","                            boxdata = []\n","                            for name in names:\n","                                if use[name]:\n","                                    if CD:\n","                                        ID = name+\"K\"+str(K)+\"exp90mass_cd\"+data_name+\"epsilon\"+str(epsilon)+\"rho\"+str(round(r,3))\n","                                    else:\n","                                        ID = name+\"K\"+str(K)+\"exp90mass\"+data_name+\"epsilon\"+str(epsilon)\n","                                        ident = \"K\"+str(K)+\"exp90mass\"+data_name+\"CDeach\"+str(cd_per)\n","                                    with open(os.path.join(folder,ID), 'rb') as input:\n","                                        boxdata.append(pickle.load(input))\n","                            ident = data_name+\" K = \"+str(K)+\" \"+r\"$\\rho$\"+\" = \"+str(round(r,3))+\" \"+r\"$\\epsilon$\"+\" = \"+str(epsilon)\n","                            rho = [float((thresh/epsilon)**(1./(t*cd_per)))]\n","                            \n","                            if EficiencyExperiment:\n","                                kmBoxplot(boxdata,xaxis,plot=plot_list,identity=ident,scale=scale_dict,folder = os.path.join(parent(path),\"Results\",data_name), \n","                                          alpha=2,cd_per = cd_per,rho_list = rho, showfliers = False,figsize = (15,6),epsilon = epsilon)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WOkjB7MDGDN"},"source":["#Returns a list containing indexes of the \"same\" moment of each concept, modulo the period cd_per.\n","#The variable des is equal to 1 when elapsed time is plotted, because of how the data was stored a shift of one unit was needed.\n","def index(i,cd_per,maxi,des=0):\n","    return np.array([i+x*cd_per+des for x in range((maxi-des)//cd_per)])\n","\n","#This function normalizes the data. The dictionary \"use\" contains which methods to normalize. \"names\" contains all possible method names. \n","#\"plot\" variable is a list containing which measures to plot and the dict \"n\" is a dictionary relating each measurement with its normalization method.\n","#\"data_name\" is the name of the dataset to be normalized, \"cd_per\" is the concept drift period and \"des\" is the index shift needed to plot elapsed time.\n","def normalize_data(data,use={},names = [],plot = [],data_name = \"\",cd_per=10,des = 0,n = {}):\n","    for pl in plot:\n","        if n[pl]:\n","            for ind in range(cd_per):\n","                gen = False\n","                for i in range(len(data[data_name])):\n","                    if use[data[data_name][i][\"name\"]]:\n","                        maxi = data[data_name][i][pl].shape[1]\n","                        if gen:\n","                            aux = np.concatenate((aux,data[data_name][i][pl][:,index(ind,cd_per,maxi,des = des)]),axis = 0)\n","                        else:\n","                            gen = True\n","                            aux = data[data_name][i][pl][:,index(ind,cd_per,maxi,des = des)]\n","                m = np.min(aux,axis = 0)\n","                for i in range(len(data[data_name])):\n","                    if use[data[data_name][i][\"name\"]]:\n","                        if n[pl] == 1:\n","                            data[data_name][i][pl][:,index(ind,cd_per,maxi,des = des)]=(data[data_name][i][pl][:,index(ind,cd_per,maxi,des = des)]-m)/m \n","                        elif n[pl] == 2:\n","                            data[data_name][i][pl][:,index(ind,cd_per,maxi,des = des)]=(data[data_name][i][pl][:,index(ind,cd_per,maxi,des = des)])/m \n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8t4evuvDGDN"},"source":["## Plot using matplotlib ##\n","\n","b = \"500\" #Parameter to differentiate each plot\n","\n","#Make the directory for results\n","os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\",\"All\"+b))) \n","for name in plot_list:\n","        os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\",\"All\"+b,name)))\n","        \n","#Go through all parameters and plot\n","for epsilon in e_list:\n","    for K in k_list:\n","        if not k_from_list:\n","            K = k_dict[data_name]\n","        for b_size in [500]:\n","            for cd_per in cd_list:\n","                xaxis = {\"t\":list(range(cd_per))}\n","                for CD in [True]:\n","                    for m in m_list:\n","                        thresh = d_thresh\n","                        r = float((thresh/epsilon)**(m/cd_per))\n","                        boxdata = dict()\n","                        data = dict()\n","                        data = dict()\n","                        data[\"All\"] = [{} for _ in range(how_many(use))]\n","                        gene = dict()\n","                        for name in names:\n","                            if use[name]:\n","                                gene[name] = False\n","                        for data_name in data_name_list:\n","                            boxdata[data_name] = []\n","                            for name in names:\n","                                if use[name]:\n","                                    if CD:\n","                                        ID = name+\"K\"+str(K)+\"exp90mass_cd\"+data_name+\"epsilon\"+str(epsilon)+\"rho\"+str(round(r,3))\n","                                    else:\n","                                        ID = name+\"K\"+str(K)+\"exp90mass\"+data_name+\"epsilon\"+str(epsilon)\n","                                        ident = \"K\"+str(K)+\"exp90mass\"+\"CDeach\"+str(cd_per)\n","                                    with open(os.path.join(folder+b,ID), 'rb') as input:\n","                                        boxdata[data_name].append(pickle.load(input))\n","                            normalize_data(boxdata,use,names,plot_list,data_name)\n","                            j = 0\n","                            for name in names:\n","                                if use[name]:\n","                                    \n","                                    if gene[name]:\n","                                        for plo in plot_list:\n","                                            data[\"All\"][j][plo] = np.concatenate((data[\"All\"][j][plo],boxdata[data_name][np.where(np.array([boxdata[data_name][i][\"name\"] for i in range(len(boxdata[data_name]))])==name)[0][0]][plo]),axis = 1)\n","                                    else:\n","                                        gene[name] = True\n","                                        data[\"All\"][j][\"name\"] = name\n","                                        for plo in plot_list:\n","                                            data[\"All\"][j][plo] = boxdata[data_name][np.where(np.array([boxdata[data_name][i][\"name\"] for i in range(len(boxdata[data_name]))])==name)[0][0]][plo]\n","                                    j+=1   \n","                            ident = \" K = \"+str(K)+\" \"+r\"$\\rho$\"+\" = \"+str(round(r,3))+\" \"+r\"$\\epsilon$\"+\" = \"+str(epsilon)\n","                            save = \"K=\"+str(K)+\"rho\"+\"=\"+str(round(r,3))+\"epsilon\"+\"=\"+str(epsilon)\n","                            rho = [float((thresh/epsilon)**(1./(t*cd_per)))]\n","                        if EficiencyExperiment:\n","                                kmBoxplot(data[\"All\"],xaxis,plot=plot_list,identity=ident,save = save,scale=scale_dict,folder = os.path.join(parent(path),\"Results\",\"All\"+b), \n","                                          alpha=2,cd_per = cd_per,rho_list = rho, showfliers = False,figsize = (15,6),epsilon = epsilon)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5LgxcihDGDN"},"source":["## Save normalized data to plot with Rstudio(ggplot) ##\n","\n","#Method names\n","use[\"SSKMI\"] = False\n","names = [\"Minimize SKM error\",\"Use previous centers\",\"New batch initialization\",\"Forget centers(Forgy)\",\"MB 100\",\"MB 1000\",\"kmc2\",\"Use previous\"\n","         ,\"SSKMI(H)\",\"SSKMI\",\"Fix centers\",\"Nearest points\",\"WSKMI\"]\n","n_dict = {\"Minimize SKM error\":\"PSKM\",\"Use previous centers\":\"UPC\",\"New batch initialization\":\"ICB\",\"SSKMI(H)\":\"HI\",\"SSKMI\":\"SI\",\"WSKMI\":\"WKI\"}\n","use[names[0]]= True\n","\n","plot_list = [\"error_real_conver\",\"error_real_init\"] #Choose which measures to plot #HEREEEE(poner metodos,arreglar lo del desplazamiento)\n","normalize_dict = {\"time\":2,\"n_dis\":2,\"n_iter\":0,\"error_init\":1,\"error_conver\":1,\"error_real_conver\":1,\"error_real_init\":1} #Dict relating each mathod to its normalization strategy\n","D = 1 #Shift parameter(only =1 for elapsed time)\n","b = \"\"#Differntiate HEREEE(mirar si se puede quitar este parmetro)\n","\n","#Make directories\n","os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\",\"All\"+b)))   \n","for name in plot_list:\n","    os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\",\"All\"+b,name)))\n","\n","#Parameters\n","xlab = [0,1,3,9] #X axis indexes to be plotted\n","k_list = [5,10,25] #K list\n","e_list = [0.5,1,2] #epsilon list\n","m_list = [1,2,3] #m list\n","\n","#Go through all parameters and save, to plot in Rstudio\n","for m in m_list:\n","    df = dict()\n","    for plot in plot_list:\n","        df[plot]=pd.DataFrame(columns=[\"value\",\"K\",\"epsilon\",\"t\",\"method\"])\n","    for epsilon in e_list:\n","        for K in k_list:\n","            if not k_from_list:\n","                K = k_dict[data_name]\n","            thresh = d_thresh\n","            r = float((thresh/epsilon)**(m/cd_per))\n","            for b_size in [500]:\n","                for cd_per in cd_list:\n","                    xaxis = {\"t\":list(range(cd_per))}\n","                    for CD in [True]:\n","                        boxdata = dict()\n","                        data = dict()\n","                        data = dict()\n","                        data[\"All\"] = [{} for _ in range(how_many(use))]\n","                        gene = dict()\n","                        for name in names:\n","                            if use[name]:\n","                                gene[name] = False\n","                        for data_name in data_name_list:\n","                            boxdata[data_name] = []\n","                            for name in names:\n","                                if use[name]:\n","                                    if CD:\n","                                        ID = name+\"K\"+str(K)+\"exp90mass_cd\"+data_name+\"epsilon\"+str(epsilon)+\"rho\"+str(round(r,3))\n","                                    else:\n","                                        ID = name+\"K\"+str(K)+\"exp90mass\"+data_name+\"epsilon\"+str(epsilon)\n","                                        ident = \"K\"+str(K)+\"exp90mass\"+\"CDeach\"+str(cd_per)\n","                                    with open(os.path.join(folder+b,ID), 'rb') as input:\n","                                        boxdata[data_name].append(pickle.load(input))\n","                            normalize_data(boxdata,use,names,plot_list,data_name,cd_per,D,normalize_dict)\n","                            j = 0\n","                            for name in names:\n","                                if use[name]:\n","\n","                                    if gene[name]:\n","                                        for plo in plot_list:\n","                                            data[\"All\"][j][plo] = np.concatenate((data[\"All\"][j][plo],boxdata[data_name][np.where(np.array([boxdata[data_name][i][\"name\"] for i in range(len(boxdata[data_name]))])==name)[0][0]][plo][:,D:]),axis = 1)\n","                                    else:\n","                                        gene[name] = True\n","                                        data[\"All\"][j][\"name\"] = name\n","                                        for plo in plot_list:\n","                                            data[\"All\"][j][plo] = boxdata[data_name][np.where(np.array([boxdata[data_name][i][\"name\"] for i in range(len(boxdata[data_name]))])==name)[0][0]][plo][:,D:]\n","                                    j+=1 \n","                            ident = \" K = \"+str(K)+\" \"+r\"$\\rho$\"+\" = \"+str(round(r,3))+\" \"+r\"$\\epsilon$\"+\" = \"+str(epsilon)\n","                            save = \"K=\"+str(K)+\"rho\"+\"=\"+str(round(r,3))+\"epsilon\"+\"=\"+str(epsilon)\n","                            rho = [float((thresh/epsilon)**(1./(t*cd_per)))]\n","            for name in names:\n","                if use[name]: \n","                    data_dict = data[\"All\"][np.where(np.array([data[\"All\"][i][\"name\"] for i in range(len(boxdata[data_name]))])==name)[0][0]]\n","                    for plot in plot_list:\n","                        for x in xlab:\n","                            aux = data_dict[plot][:,index(x,cd_per,data_dict[plot].shape[1],0)]\n","                            aux = np.concatenate((aux,np.array([[K for _ in range(aux.shape[1])]]),np.array([[epsilon for _ in range(aux.shape[1])]]),np.array([[x for _ in range(aux.shape[1])]])+1),axis = 0)\n","                            aux_df = pd.DataFrame(aux.transpose(),columns=[\"value\",\"K\",\"epsilon\",\"t\"])\n","                            aux_df[\"method\"] = n_dict[name]\n","                            df[plot] = df[plot].append(aux_df)\n","    for plot in plot_list:\n","        df[plot].to_csv(os.path.join(MAIN,\"Rstudio_csv\",\"plot\"+str(plot)+\"rho_speed\"+str(t)),index = False)  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERl-83tyDGDN"},"source":[""],"execution_count":null,"outputs":[]}]}